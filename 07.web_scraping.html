<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta charset="utf-8">
  <title>웹 스크래핑</title>

  <link rel="stylesheet" href="style.css" />
  <style>
    pre {
      font-family: "맑은 고딕";
    }
  </style>
</head>

<body>

  <div id="page">
    <div id="header">
      <a href="00_index.html">지각생의 웹세상. </a>

    </div>

    <div id="main">
      <ol type="I">
        <li>
          <h2><a id="index 1">request</a></h2>
        </li>

        <pre>
import requests

headers = {"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36"}

url = "http://nadocoding.tistory.com"
res = requests.get(url, headers=headers)
res.raise_for_status()
print("응답코드 :", res.status_code) # 200 이면 정상

if res.status_code == requests.codes.ok:
    print("정상입니다")
else:
    print("문제가 생겼습니다. [에러코드 ", res.status_code, "]")

print(len(res.text))
print(res.text)

with open("mygoogle.html", "w", encoding="utf8") as f:
    f.write(res.text)

</pre>


        <li>
          <h2><a id="index 2">re</a></h2>
        </li>
        <pre>
import re
# abcd, book, desk
# ca?e
# care, cafe, case, cave
# caae, cabe, cace, cade, ...

p = re.compile("ca.e")
# . (ca.e) : 하나의 문자를 의미 > care, cafe, case (O) | caffe (X)
# ^ (^de)  : 문자열의 시작 > desk, destination (O) | fade (X)
# $ (se$)  : 문자열의 끝 > case, base (O) | face (X)

def print_match(m):
    if m:
        print("m.group():", m.group()) # 일치하는 문자열 반환
        print("m.string:", m.string) # 입력받은 문자열
        print("m.start():", m.start()) # 일치하는 문자열의 시작 index
        print("m.end():", m.end()) # 일치하는 문자열의 끝 index
        print("m.span():", m.span()) # 일치하는 문자열의 시작 / 끝 index
    else:
        print("매칭되지 않음")

# m = p.match("careless") # match : 주어진 문자열의 처음부터 일치하는지 확인
# print_match(m)

# m = p.search("good care") # search : 주어진 문자열 중에 일치하는게 있는지 확인
# print_match(m)

# lst = p.findall("good care cafe") # findall : 일치하는 모든 것을 리스트 형태로 반환
# print(lst)


# 1. p = re.compile("원하는 형태")
# 2. m = p.match("비교할 문자열") : 주어진 문자열의 처음부터 일치하는지 확인
# 3. m = p.search("비교할 문자열") : 주어진 문자열 중에 일치하는게 있는지 확인
# 4. lst = p.findall("비교할 문자열") : 일치하는 모든 것을 "리스트" 형태로 반환

# 원하는 형태 : 정규식
# . (ca.e) : 하나의 문자를 의미 > care, cafe, case (O) | caffe (X)
# ^ (^de)  : 문자열의 시작 > desk, destination (O) | fade (X)
# $ (se$)  : 문자열의 끝 > case, base (O) | face (X)
        </pre>



        <li>
          <h2><a id="index 3">bs4</a></h2>
        </li>
        <pre>
  import requests
  import re
  from bs4 import BeautifulSoup

  url ="https://comic.naver.com/webtoon/weekday.nhn"
  res = requests.get(url)
  res.raise_for_status()
  soup = BeautifulSoup(res.text, "lxml")                                         #res변수로 가져온 text를 lxml를 통해서 beautifulsoup객체로 만든것이다.

  items = soup.find_all("li", attrs={"class":re.compile("^search-product")})     #정규식으로 클래스명 찾아서 요소찾기

  print(soup.title)                                                              #해당 페이지의 title과 같다 즉 해당 페이지 엘리먼트에 접근할 수 있게 된다
  print(soup.title.get_text())                                                   #text만 가져오기 (태그 정보들 생략)   
  print(soup.a)                                                                  # soup 객체에서 처음 발견되는 a element 출력
  print(soup.a.attrs)                                                            # a element 의 속성 정보를 출력
  print(soup.a["href"])                                                          # a element 의 href 속성 '값' 정보를 출력

  print(soup.find("a", attrs={"class":"Nbtn_upload"}))                           # class="Nbtn_upload" 인 a element 를 찾아줘. 딕셔너리 개념
  print(soup.find(attrs={"class":"Nbtn_upload"}))                                # class="Nbtn_upload" 인 어떤 element 를 찾아줘. 딕셔너리 개념

  cartoons = soup.find_all("a", attrs={"class":"title"})                         # class 속성이 title 인 모든 "a" element 를 반환

  for cartoon in cartoons:                                                       # 만화 제목 + 링크 가져오기
      title = cartoon.a.get_text()
      link = "https://comic.naver.com" + cartoon.a["href"]
      print(title, link)


  total_rates = 0                                                                # 평점 구하기
  cartoons = soup.find_all("div", attrs={"class":"rating_type"})
  for cartoon in cartoons:
      rate = cartoon.find("strong").get_text()                                   # find("strong") 이런 방식으로도 태그를 찾아서 요소 정보 찾을수 있음.   
      print(rate)
      total_rates += float(rate)
  print("전체 점수 : ", total_rates)
  print("평균 점수 : ", total_rates / len(cartoons))

  rank1 = soup.find("li", attrs={"class":"rank01"})
  print(rank1.a.get_text())
  print(rank1.next_sibling)                                                      # 예를들어 <li><pre>의 연속 같은 형제들 간의 다음 형제 태그 요소 찾기
  rank2 = rank1.next_sibling.next_sibling
  rank3 = rank2.next_sibling.next_sibling
  print(rank3.a.get_text())
  rank2 = rank3.previous_sibling.previous_sibling
  print(rank2.a.get_text())
  print(rank1.parent)
  rank2 = rank1.find_next_sibling("li")
  print(rank2.a.get_text())
  rank3 = rank2.find_next_sibling("li")
  print(rank3.a.get_text())
  rank2 = rank3.find_previous_sibling("li")
  print(rank2.a.get_text())

  print(rank1.find_next_siblings("li"))                                           #다음 li를 가진 형제들 모두 찾음

  webtoon = soup.find("a", text="독립일기-11화 밥공기 딜레마")                      # 열고 닫는 태그 사이의 글자가 text부분이다
  print(webtoon)

        </pre>



        <li>
          <h2><a id="index 4">selenium</a></h2>
        </li>
        <pre>
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys


&lt;!-- 
#브라우저 창 안띄우고 백그라운드에서 작업하기
options = webdriver.ChromeOptions()
options.headless = True
options.add_argument("window-size=1920x1080")
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36")

browser = webdriver.Chrome(options=options)

browser.get_screenshot_as_file("google_movie.png") #백그라운드에서 잘 작동되는지 확인 원하면 스샷 가능

--&gt;


browser = webdriver.Chrome(r"C:\Users\Son\Desktop\IT공부\web-1\web1\05_web_scraping\03_saramin_selenium\chromedriver.exe")     # "./chromedriver.exe"

headers = {
  "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36",
  "Accept-Language":"ko-KR,ko"
  }
browser.get("http://naver.com")
elem = browser.find_element_by_class_name("link_login")
elem.click()


# 1. 네이버 이동
browser.get("http://naver.com")

# 2. 로그인 버튼 클릭
elem = browser.find_element_by_class_name("link_login")
elem.click()

# 3. id, pw 입력
browser.find_element_by_id("id").send_keys("naver_id")
browser.find_element_by_id("pw").send_keys("password")

# 4. 로그인 버튼 클릭
browser.find_element_by_id("log.login").click()

time.sleep(3)

# 5. id 를 새로 입력
#browser.find_element_by_id("id").send_keys("my_id")

browser.find_element_by_id("id").clear()
browser.find_element_by_id("id").send_keys("my_id")     #해당 라인은 id="id"인 인풋 박스에 "my_id"텍스트를 입력한다.
##from selenium.webdriver.common.keys import Keys
##elem.send_keys(Keys.ENTER)                            #해당 라인은 위에 Keys를 임포트해야 쓸 수 있다.

# 6. html 정보 출력
print(browser.page_source)

# 7. 브라우저 종료
#browser.close() # 현재 탭만 종료
browser.quit() # 전체 브라우저 종료




# 지정한 위치로 스크롤 내리기
# 모니터(해상도) 높이인 1080 위치로 스크롤 내리기
#browser.execute_script("window.scrollTo(0, 1080)") # 1920 x 1080
#browser.execute_script("window.scrollTo(0, 2080)")






# 화면 가장 아래로 스크롤 내리기
# browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
import time
interval = 2 # 2초에 한번씩 스크롤 내림
# 현재 문서 높이를 가져와서 저장
prev_height = browser.execute_script("return document.body.scrollHeight")
# 반복 수행
while True:
    # 스크롤을 가장 아래로 내림
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")

    # 페이지 로딩 대기
    time.sleep(interval)

    # 현재 문서 높이를 가져와서 저장
    curr_height = browser.execute_script("return document.body.scrollHeight")
    if curr_height == prev_height:
        break

    prev_height = curr_height

print("스크롤 완료")





# Mozilla/5.0 (Windows NT 10.0; Win64; x64) 
# AppleWebKit/537.36 (KHTML, like Gecko) 
# Chrome/84.0.4147.89 Safari/537.36
detected_value = browser.find_element_by_id("detected_value")
print(detected_value.text)





================================================================selenium

# 네이버 이동
browser.get('http://naver.com')

# 카페 메뉴 찾기
elem = browser.find_element_by_link_text('카페')

# 속성 가져오기
elem.get_attribute('href')
elem.get_attribute('class')

# 클릭
elem.click()

# 뒤로 가기
browser.back()

# 앞으로 가기
browser.forward()

# 새로고침
browser.refresh()

# 뒤로 가기
browser.back()

# 검색창 찾기 (개발자 도구 이용)
elem = browser.find_element_by_id('query')

# 글자 입력하기
elem.send_keys('나도코딩')

# enter 치기
from selenium.webdriver.common.keys import Keys
elem.send_keys(Keys.ENTER)

# a 태그 찾기
elem = browser.find_element_by_tag_name('a')
elem.get_attribute('href')

# a 태그 모두 찾기
elems = browser.find_elements_by_tag_name('a')
for e in elems:
    e.get_attribute('href')

# 다음으로 이동
browser.get('http://daum.net')

# 검색창 찾기
elem = browser.find_element_by_name('q')

# 글자 입력하기
elem.send_keys("나도코딩")

# 글자 지우기
elem.clear()

# 글자 입력하기
elem.send_keys("나도코딩")

# 검색 버튼 찾기
elem = browser.find_element_by_xpath('//*[@id="daumSearch"]/fieldset/div/div/button[2]')

# 클릭하기
elem.click()

# 스크린샷 찍기
browser.save_screenshot('daum.png')

# 페이지 소스 보기
browser.page_source

# 브라우저 종료
browser.close() # 현재 탭 닫기
browser.quit() # 브라우저 종료하기

================================================================iframe
import time
from selenium import webdriver

browser = webdriver.Chrome()

browser.get('https://www.w3schools.com/tags/tryit.asp?filename=tryhtml5_input_type_radio')

browser.switch_to.frame('iframeResult') # frame 전환

elem = browser.find_element_by_xpath('//*[@id="male"]') # 성공

elem.click()

browser.switch_to.default_content() # 상위로 빠져 나옴

elem = browser.find_element_by_xpath('//*[@id="male"]') # 실패

elem.click()

time.sleep(5) # 5초 대기

browser.quit()

#//*[@id="male"]

================================================================radio_button
import time
from selenium import webdriver

browser = webdriver.Chrome()
browser.maximize_window() # 창 최대화

browser.get('https://www.w3schools.com/tags/tryit.asp?filename=tryhtml5_input_type_radio')

browser.switch_to.frame('iframeResult') # frame 전환

elem = browser.find_element_by_xpath('//*[@id="male"]')

# 선택이 안되어 있으면 선택하기
if elem.is_selected() == False: # 라디오 버튼이 선택되어 있지 않으면
    print("선택 안되어 있으므로 선택하기")
    elem.click()
else: # 라디오 버튼이 선택되어 있다면
    print("선택 되어 있으므로 아무것도 안함")

time.sleep(5) # 5초 대기

# 선택이 안되어 있으면 선택하기
if elem.is_selected() == False: # 라디오 버튼이 선택되어 있지 않으면
    print("선택 안되어 있으므로 선택하기")
    elem.click()
else: # 라디오 버튼이 선택되어 있다면
    print("선택 되어 있으므로 아무것도 안함")

browser.quit()

================================================================check box
import time
from selenium import webdriver
from selenium.webdriver.common.by import By

browser = webdriver.Chrome()
browser.maximize_window()

browser.get('https://www.w3schools.com/tags/tryit.asp?filename=tryhtml5_input_type_checkbox')

browser.switch_to.frame('iframeResult')

#elem = browser.find_element_by_xpath('//*[@id="vehicle1"]')
#elem = browser.find_element(By.XPATH, '//*[@id="vehicle1"]')
elem = browser.find_element(By.ID, 'vehicle1')

time.sleep(5)

if elem.is_selected() == False:
    print("선택 안되어 있으므로 선택")
    elem.click()
else:
    print("선택 되어 있으므로 아무것도 안함")

time.sleep(5)
browser.quit()
================================================================select_option
import time
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_option')

browser.switch_to.frame('iframeResult')

# cars 에 해당하는 element 를 찾고, 드롭다운 내부에 있는 4번째 옵션을 선택
# elem = browser.find_element_by_xpath('//*[@id="cars"]/option[4]')
# option[1] : 첫번째 항목 
# option[2] : 두번째 항목 
# ...
# elem.click()

time.sleep(5)

# 완전히 일치하는 텍스트 값을 통해서 선택하는 방법
# 옵션 중에서 텍스트가 Audi 인 항목을 선택
# elem = browser.find_element_by_xpath('//*[@id="cars"]/option[text()="Audi"]')
# elem.click()

# 텍스트 값이 부분 일치하는 항목 선택하는 방법
elem = browser.find_element_by_xpath('//*[@id="cars"]/option[contains(text(), "Au")]')
elem.click()

time.sleep(5)

browser.quit()
================================================================page_scroll
import time
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

browser = webdriver.Chrome()
browser.maximize_window()

browser.get('https://shopping.naver.com/home/p/index.nhn')

# '무선마우스' 입력
elem = browser.find_element_by_xpath('//*[@id="autocompleteWrapper"]/input[1]')
elem.send_keys('무선마우스')

time.sleep(1)

elem.send_keys(Keys.ENTER) # 검색 버튼 클릭을 위해 Enter 처리

# 스크롤
# 지정한 위치로 스크롤 내리기
# 모니터 (해상도) 높이인 1080 위치로 스크롤 내리기
# browser.execute_script('window.scrollTo(0, 1080)') # 1920 * 1080 (모니터 해상도)
# browser.execute_script('window.scrollTo(0, 2080)')

# 화면 가장 아래로 스크롤 내리기
# browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')


# 동적 페이지에 대해서 마지막까지 스크롤 반복 수행
interval = 2 # 2초에 한번씩 스크롤 내리기

# 현재 문서 높이를 가져와서 저장
prev_height = browser.execute_script('return document.body.scrollHeight')

# 반복 수행
while True:
    # 스크롤을 화면 가장 아래로 내림
    browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')

    # 페이지 로딩 대기 (2초)
    time.sleep(interval)

    # 현재 문서 높이 가져와서 저장
    curr_height = browser.execute_script('return document.body.scrollHeight')
    if curr_height == prev_height: # 직전 높이와 같으면, 높이 변화가 없으면
        break # 반복문 탈출 (모든 스크롤 동작 완료)

    prev_height = curr_height

# 맨 위로 올리기
browser.execute_script('window.scrollTo(0, 0)')

time.sleep(5)
browser.quit()
================================================================scroll_nested
import time
from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains

browser = webdriver.Chrome()
browser.get('https://www.w3schools.com/html/')
browser.maximize_window()

time.sleep(5)

# 특정 영역 스크롤
elem = browser.find_element_by_xpath('//*[@id="leftmenuinnerinner"]/a[61]')

# 방법 1 : ActionChain
# actions = ActionChains(browser)
# actions.move_to_element(elem).perform()

# 방법 2 : 좌표 정보 이용
# xy = elem.location_once_scrolled_into_view # 함수가 아니니까 () 쓰지 마세요
# print("type : ", type(xy))  # dict
# print("value : ", xy)

elem.click()

time.sleep(5)
browser.quit()
================================================================file_download
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_experimental_option('prefs', {'download.default_directory':r'C:\Users\Nadocoding\Desktop\PythonWorkspace'})

browser = webdriver.Chrome(options=chrome_options)
browser.get('https://www.w3schools.com/tags/tryit.asp?filename=tryhtml5_a_download')

browser.switch_to.frame('iframeResult')

# download 링크 클릭
elem = browser.find_element_by_xpath('/html/body/p[2]/a')
elem.click()

time.sleep(5)
browser.quit()
================================================================wait
import time
from selenium import webdriver

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

browser = webdriver.Chrome()
browser.maximize_window()
browser.get('https://flight.naver.com/flights/')

# 가는 날 클릭
browser.find_element_by_link_text('가는날 선택').click()
browser.find_elements_by_link_text('30')[0].click()
# 오는 날
browser.find_elements_by_link_text('5')[1].click()

# 제주도 클릭
browser.find_element_by_xpath('//*[@id="recommendationList"]/ul/li[1]').click()

# 항공권 검색 클릭
browser.find_element_by_link_text('항공권 검색').click()

# time.sleep(10)

try:
    elem = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id="content"]/div[2]/div/div[4]/ul/li[1]')))
    print(elem.text)
except:
    print("실패했어요")

# 첫 번째 결과 출력
# elem = browser.find_element_by_xpath('//*[@id="content"]/div[2]/div/div[4]/ul/li[1]')
# print(elem.text) # element 내에 있는 text 부분을 출력

time.sleep(5)
browser.quit()
================================================================handle
import time
from selenium import webdriver

browser = webdriver.Chrome()
browser.maximize_window()

browser.get('https://www.w3schools.com/tags/att_input_type_radio.asp')
curr_handle = browser.current_window_handle
print(curr_handle) # 현재 윈도우 핸들 정보

# Try it yourself
browser.find_element_by_xpath('//*[@id="main"]/div[2]/a').click()

handles = browser.window_handles # 모든 핸들 정보
for handle in handles:
    print(handle) # 각 핸들 정보
    browser.switch_to.window(handle) # 각 핸들로 이동해서
    print(browser.title) # 출력해보면 현재 핸들 (브라우저) 의 제목 표시
    print()

# 새로 이동된 브라우저에서 뭔가 자동화 작업을 수행..

# 그 브라우저를 종료
print("현재 핸들 닫기")
browser.close() 

# 이전 핸들로 돌아오기
print("처음 핸들로 돌아오기")
browser.switch_to.window(curr_handle)

print(browser.title) # HTML input type="radio"

# 브라우저 컨트롤이 가능한지 확인
time.sleep(5)
browser.get('http://daum.net')

time.sleep(5)
browser.quit()

</pre>
      </ol>


    </div>














    <div id="sidebar">
      목록
      <ol>
        <li><a href="#index 1">requests</a></li>
        <li><a href="#index 2">re</a></li>
        <li><a href="#index 3">bs4</li>
        <li><a href="#index 4">selenium</a></li>
        <li><a href="#index 5">__</a></li>

      </ol>
    </div>

    <div id="footer">
      다른 월드로 &nbsp;:&nbsp;
      <a href="01_html5.html">HTML5 &nbsp;&nbsp;&nbsp;&nbsp;</a>
      <a href="02_css3.html">CSS3 &nbsp;&nbsp;&nbsp;&nbsp;</a>
      <a href="03_javaScript.html">자바스크립트 &nbsp;&nbsp;&nbsp;&nbsp;</a>
      <a href="04_파이썬.html">파이썬 &nbsp;&nbsp;&nbsp;&nbsp;</a>
      <a href="05_java.html">자바 &nbsp;&nbsp;&nbsp;&nbsp;</a>
      <a href="06_라즈베리파이4.html">라즈베리파이4 &nbsp;&nbsp;&nbsp;&nbsp;</a>
      <a href="07.web_scraping.html">웹 스크래핑 &nbsp;&nbsp;&nbsp;&nbsp;</a>
    </div>
  </div>
</body>

</html>